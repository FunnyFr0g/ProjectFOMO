# evaluate_model_metrics.py
import os
import sys
import argparse
from pathlib import Path
import logging
from datetime import datetime

import torch
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
from sklearn.metrics import (
    roc_auc_score, roc_curve,
    precision_recall_curve, average_precision_score,
    confusion_matrix, classification_report,
    accuracy_score, f1_score, precision_score, recall_score
)
from sklearn.calibration import calibration_curve

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ —Ç–µ–∫—É—â–µ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –¥–ª—è –∏–º–ø–æ—Ä—Ç–∞
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

# –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –∏–∑ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–≥–æ —Ñ–∞–π–ª–∞
from TF_encoder_mobilenet import (
    Config,
    DroneBirdDataset,
    get_transforms,
    SequenceEncoder,
    DataLoader
)
import torch.nn.functional as F


class ModelEvaluator:
    def __init__(self, model_path, config=None, results_dir=None):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –æ—Ü–µ–Ω—â–∏–∫–∞ –º–æ–¥–µ–ª–∏

        Args:
            model_path: –ø—É—Ç—å –∫ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ (.pth —Ñ–∞–π–ª)
            config: –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è (–µ—Å–ª–∏ None, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è Config –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é)
            results_dir: –ø–∞–ø–∫–∞ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ (–µ—Å–ª–∏ None, —Å–æ–∑–¥–∞–µ—Ç—Å—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏)
        """
        self.model_path = Path(model_path)
        if not self.model_path.exists():
            raise FileNotFoundError(f"–§–∞–π–ª –º–æ–¥–µ–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω: {model_path}")

        # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
        if config is None:
            self.config = Config()
        else:
            self.config = config

        # –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        logger.info(f"–ò—Å–ø–æ–ª—å–∑—É–µ–º–æ–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {self.device}")

        # –ü–∞–ø–∫–∞ –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        if results_dir is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            self.results_dir = Path("evaluation_results") / f"{self.model_path.stem}_{timestamp}"
        else:
            self.results_dir = Path(results_dir)

        self.results_dir.mkdir(parents=True, exist_ok=True)
        logger.info(f"–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –±—É–¥—É—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {self.results_dir}")

        # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏ –¥–∞–Ω–Ω—ã—Ö
        self.model = None
        self.val_loader = None
        self._load_model()
        self._prepare_data()

    def _load_model(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏–∑ —á–µ–∫–ø–æ–∏–Ω—Ç–∞"""
        logger.info(f"–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏–∑ {self.model_path}")

        # –°–æ–∑–¥–∞–µ–º –º–æ–¥–µ–ª—å —Å —Ç–µ–º–∏ –∂–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
        self.model = SequenceEncoder(
            embedding_dim=self.config.embedding_dim,
            dropout_rate=self.config.dropout_rate
        ).to(self.device)

        # –ó–∞–≥—Ä—É–∂–∞–µ–º –≤–µ—Å–∞
        checkpoint = torch.load(self.model_path, map_location=self.device)

        if 'model_state_dict' in checkpoint:
            self.model.load_state_dict(checkpoint['model_state_dict'])
        else:
            # –ü—ã—Ç–∞–µ–º—Å—è –∑–∞–≥—Ä—É–∑–∏—Ç—å –Ω–∞–ø—Ä—è–º—É—é
            self.model.load_state_dict(checkpoint)

        self.model.eval()
        logger.info("–ú–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")

    def _prepare_data(self):
        """–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞"""
        logger.info("–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞...")

        try:
            # –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏ –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏
            val_transform = get_transforms(is_train=False)

            # –°–æ–∑–¥–∞–µ–º –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç
            val_dataset = DroneBirdDataset(
                root_dir=self.config.data_dir,
                transform=val_transform,
                sequence_length=self.config.sequence_length,
                is_train=False,
                split_ratio=0.8
            )

            if len(val_dataset) == 0:
                logger.error("–í–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç –ø—É—Å—Ç!")
                self.val_loader = None
                return

            # DataLoader
            self.val_loader = DataLoader(
                val_dataset,
                batch_size=self.config.batch_size,
                shuffle=False,
                num_workers=0,  # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º 0 –¥–ª—è –∏–∑–±–µ–∂–∞–Ω–∏—è –ø—Ä–æ–±–ª–µ–º
                pin_memory=True
            )

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –¥–∞—Ç–∞—Å–µ—Ç–µ
            self.val_labels = np.array(val_dataset.labels)
            self.num_samples = len(val_dataset)
            self.num_birds = (self.val_labels == 0).sum()
            self.num_drones = (self.val_labels == 1).sum()

            logger.info(f"–í–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç: {self.num_samples} –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π")
            logger.info(f"  - –ü—Ç–∏—Ü—ã: {self.num_birds}")
            logger.info(f"  - –î—Ä–æ–Ω—ã: {self.num_drones}")

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –æ–±–æ–∏—Ö –∫–ª–∞—Å—Å–æ–≤
            unique_labels = np.unique(self.val_labels)
            if len(unique_labels) < 2:
                logger.warning(f"–¢–æ–ª—å–∫–æ –æ–¥–∏–Ω –∫–ª–∞—Å—Å –≤ –¥–∞–Ω–Ω—ã—Ö: {unique_labels}")

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–µ –¥–∞–Ω–Ω—ã—Ö: {e}")
            self.val_loader = None

    def get_predictions(self):
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –º–æ–¥–µ–ª–∏ –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–æ–º –¥–∞—Ç–∞—Å–µ—Ç–µ"""
        logger.info("–ü–æ–ª—É—á–µ–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –º–æ–¥–µ–ª–∏...")

        if self.val_loader is None:
            logger.error("DataLoader –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω!")
            return np.array([]), np.array([]), np.array([])

        self.model.eval()
        all_labels = []
        all_preds = []
        all_probs = []

        with torch.no_grad():
            for batch_idx, batch in enumerate(self.val_loader):
                try:
                    frames = batch['frames'].to(self.device)
                    labels = batch['label'].to(self.device)

                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–º–µ—Ä –±–∞—Ç—á–∞
                    if frames.shape[0] == 0:
                        continue

                    # Forward pass
                    _, class_logits = self.model(frames, return_classification=True)

                    # –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏
                    probs = F.softmax(class_logits, dim=1)

                    # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–µ –∫–ª–∞—Å—Å—ã
                    preds = torch.argmax(class_logits, dim=1)

                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º
                    all_labels.append(labels.cpu().numpy())
                    all_preds.append(preds.cpu().numpy())
                    all_probs.append(probs.cpu().numpy())

                    if batch_idx % 10 == 0:
                        logger.info(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ –±–∞—Ç—á–µ–π: {batch_idx + 1}/{len(self.val_loader)}")

                except Exception as e:
                    logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –±–∞—Ç—á–∞ {batch_idx}: {e}")
                    continue

        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ –±–∞—Ç—á–∏, –ø—Ä–æ–≤–µ—Ä—è—è —á—Ç–æ –µ—Å—Ç—å –¥–∞–Ω–Ω—ã–µ
        if len(all_labels) == 0:
            logger.error("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –ø–æ—Å–ª–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏!")
            return np.array([]), np.array([]), np.array([])

        try:
            y_true = np.concatenate(all_labels)
            y_pred = np.concatenate(all_preds)
            y_proba = np.concatenate(all_probs)

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏
            if len(y_true) != len(y_pred) or len(y_true) != y_proba.shape[0]:
                logger.error(
                    f"–†–∞–∑–º–µ—Ä—ã –Ω–µ —Å–æ–≤–ø–∞–¥–∞—é—Ç! y_true={len(y_true)}, y_pred={len(y_pred)}, y_proba={y_proba.shape[0]}")
                # –ò—Å–ø—Ä–∞–≤–ª—è–µ–º - –±–µ—Ä–µ–º –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä
                min_len = min(len(y_true), len(y_pred), y_proba.shape[0])
                y_true = y_true[:min_len]
                y_pred = y_pred[:min_len]
                y_proba = y_proba[:min_len]

            logger.info(f"–ü–æ–ª—É—á–µ–Ω–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π: {len(y_true)}")
            logger.info(f"–†–∞–∑–º–µ—Ä y_proba: {y_proba.shape}")

            return y_true, y_pred, y_proba

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–∏ –¥–∞–Ω–Ω—ã—Ö: {e}")
            return np.array([]), np.array([]), np.array([])

    def calculate_metrics(self, y_true, y_pred, y_proba):
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ –≤—Å–µ—Ö –º–µ—Ç—Ä–∏–∫"""
        logger.info("–í—ã—á–∏—Å–ª–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫...")

        metrics = {}

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ –¥–∞–Ω–Ω—ã–µ
        if len(y_true) == 0:
            logger.error("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –º–µ—Ç—Ä–∏–∫!")
            metrics['error'] = "No data available"
            return metrics

        # –û—Å–Ω–æ–≤–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
        metrics['accuracy'] = accuracy_score(y_true, y_pred)

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –æ–±–æ–∏—Ö –∫–ª–∞—Å—Å–æ–≤ –¥–ª—è –±–∏–Ω–∞—Ä–Ω—ã—Ö –º–µ—Ç—Ä–∏–∫
        unique_labels = np.unique(y_true)
        if len(unique_labels) == 2:
            metrics['precision'] = precision_score(y_true, y_pred, average='binary', zero_division=0)
            metrics['recall'] = recall_score(y_true, y_pred, average='binary', zero_division=0)
            metrics['f1_score'] = f1_score(y_true, y_pred, average='binary', zero_division=0)
        else:
            logger.warning(f"–û–±–Ω–∞—Ä—É–∂–µ–Ω —Ç–æ–ª—å–∫–æ –æ–¥–∏–Ω –∫–ª–∞—Å—Å: {unique_labels}. –ò—Å–ø–æ–ª—å–∑—É–µ–º 'macro' averaging.")
            metrics['precision'] = precision_score(y_true, y_pred, average='macro', zero_division=0)
            metrics['recall'] = recall_score(y_true, y_pred, average='macro', zero_division=0)
            metrics['f1_score'] = f1_score(y_true, y_pred, average='macro', zero_division=0)

        # ROC-AUC –∏ Precision-Recall AUC
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –µ—Å—Ç—å –æ–±–∞ –∫–ª–∞—Å—Å–∞
            if len(np.unique(y_true)) >= 2:
                metrics['roc_auc'] = roc_auc_score(y_true, y_proba[:, 1])
                metrics['pr_auc'] = average_precision_score(y_true, y_proba[:, 1])
            else:
                logger.warning("–¢–æ–ª—å–∫–æ –æ–¥–∏–Ω –∫–ª–∞—Å—Å –≤ –¥–∞–Ω–Ω—ã—Ö, AUC –Ω–µ –º–æ–∂–µ—Ç –±—ã—Ç—å –≤—ã—á–∏—Å–ª–µ–Ω")
                metrics['roc_auc'] = 0.0
                metrics['pr_auc'] = 0.0
        except Exception as e:
            logger.warning(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã—á–∏—Å–ª–µ–Ω–∏–∏ AUC –º–µ—Ç—Ä–∏–∫: {e}")
            metrics['roc_auc'] = 0.0
            metrics['pr_auc'] = 0.0

        # –ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫
        try:
            cm = confusion_matrix(y_true, y_pred)
            metrics['confusion_matrix'] = cm.tolist()

            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –∏–∑ –º–∞—Ç—Ä–∏—Ü—ã –æ—à–∏–±–æ–∫
            if cm.size == 4:  # 2x2 –º–∞—Ç—Ä–∏—Ü–∞
                tn, fp, fn, tp = cm.ravel()
                metrics['true_positive'] = int(tp)
                metrics['true_negative'] = int(tn)
                metrics['false_positive'] = int(fp)
                metrics['false_negative'] = int(fn)

                metrics['false_positive_rate'] = fp / (fp + tn) if (fp + tn) > 0 else 0
                metrics['false_negative_rate'] = fn / (fn + tp) if (fn + tp) > 0 else 0
                metrics['true_positive_rate'] = tp / (tp + fn) if (tp + fn) > 0 else 0
                metrics['true_negative_rate'] = tn / (tn + fp) if (tn + fp) > 0 else 0
            else:
                logger.warning(f"–ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫ –∏–º–µ–µ—Ç —Ä–∞–∑–º–µ—Ä {cm.shape}, –∞ –Ω–µ 2x2")

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã—á–∏—Å–ª–µ–Ω–∏–∏ –º–∞—Ç—Ä–∏—Ü—ã –æ—à–∏–±–æ–∫: {e}")
            metrics['confusion_matrix'] = []

        # –ü–æ–¥—Ä–æ–±–Ω—ã–π –æ—Ç—á–µ—Ç
        try:
            report = classification_report(
                y_true, y_pred,
                target_names=['Bird', 'Drone'],
                output_dict=True,
                zero_division=0
            )
            metrics['classification_report'] = report
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ classification report: {e}")
            metrics['classification_report'] = {}

        # –ë–∞–∑–æ–≤–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
        metrics['num_samples'] = len(y_true)
        metrics['num_birds'] = int((y_true == 0).sum())
        metrics['num_drones'] = int((y_true == 1).sum())

        return metrics

    def plot_roc_curve(self, y_true, y_proba, roc_auc):
        """–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ ROC-–∫—Ä–∏–≤–æ–π"""
        plt.figure(figsize=(10, 8))

        fpr, tpr, thresholds = roc_curve(y_true, y_proba[:, 1])

        plt.plot(fpr, tpr, color='darkorange', lw=2,
                 label=f'ROC curve (AUC = {roc_auc:.4f})')
        plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random')

        plt.xlim([0.0, 1.0])
        plt.ylim([0.0, 1.05])
        plt.xlabel('False Positive Rate')
        plt.ylabel('True Positive Rate')
        plt.title('Receiver Operating Characteristic (ROC) Curve')
        plt.legend(loc="lower right")
        plt.grid(True, alpha=0.3)

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º
        save_path = self.results_dir / 'roc_curve.png'
        plt.savefig(save_path, dpi=150, bbox_inches='tight')
        plt.close()

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–∞–Ω–Ω—ã–µ - –û–ë–ù–û–í–õ–ï–ù–û: –∏—Å–ø—Ä–∞–≤–ª—è–µ–º —Ä–∞–∑–º–µ—Ä—ã –º–∞—Å—Å–∏–≤–æ–≤
        # –î–ª–∏–Ω–∞ thresholds –æ–±—ã—á–Ω–æ –Ω–∞ 1 –º–µ–Ω—å—à–µ, —á–µ–º fpr –∏ tpr
        if len(fpr) == len(tpr):
            if len(thresholds) == len(fpr) - 1:
                # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π –ø–æ—Ä–æ–≥ –¥–ª—è –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è —Ä–∞–∑–º–µ—Ä–æ–≤
                thresholds_adjusted = np.append(thresholds, thresholds[-1] if len(thresholds) > 0 else 1.0)
            else:
                # –ï—Å–ª–∏ —Ä–∞–∑–º–µ—Ä—ã —É–∂–µ —Å–æ–≤–ø–∞–¥–∞—é—Ç
                thresholds_adjusted = thresholds

            # –°–æ–∑–¥–∞–µ–º DataFrame —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –≤—Å–µ –º–∞—Å—Å–∏–≤—ã –æ–¥–∏–Ω–∞–∫–æ–≤–æ–π –¥–ª–∏–Ω—ã
            if len(fpr) == len(tpr) == len(thresholds_adjusted):
                roc_data = pd.DataFrame({
                    'fpr': fpr,
                    'tpr': tpr,
                    'thresholds': thresholds_adjusted
                })
                roc_data.to_csv(self.results_dir / 'roc_data.csv', index=False)
            else:
                logger.warning(
                    f"–†–∞–∑–º–µ—Ä—ã –º–∞—Å—Å–∏–≤–æ–≤ –Ω–µ —Å–æ–≤–ø–∞–¥–∞—é—Ç: fpr={len(fpr)}, tpr={len(tpr)}, thresholds={len(thresholds_adjusted)}")
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ç–¥–µ–ª—å–Ω–æ
                roc_data = pd.DataFrame({
                    'fpr': fpr,
                    'tpr': tpr
                })
                roc_data.to_csv(self.results_dir / 'roc_data.csv', index=False)
                # –ü–æ—Ä–æ–≥–∏ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ç–¥–µ–ª—å–Ω–æ
                pd.DataFrame({'thresholds': thresholds}).to_csv(
                    self.results_dir / 'roc_thresholds.csv', index=False)
        else:
            logger.warning(f"–†–∞–∑–º–µ—Ä—ã fpr –∏ tpr –Ω–µ —Å–æ–≤–ø–∞–¥–∞—é—Ç: fpr={len(fpr)}, tpr={len(tpr)}")

        logger.info(f"ROC-–∫—Ä–∏–≤–∞—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {save_path}")

        return save_path

    def plot_precision_recall_curve(self, y_true, y_proba, pr_auc):
        """–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ Precision-Recall –∫—Ä–∏–≤–æ–π"""
        plt.figure(figsize=(10, 8))

        precision, recall, thresholds = precision_recall_curve(y_true, y_proba[:, 1])

        plt.plot(recall, precision, color='blue', lw=2,
                 label=f'PR curve (AP = {pr_auc:.4f})')

        # Baseline (—Å–ª—É—á–∞–π–Ω—ã–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä)
        positive_ratio = np.mean(y_true)
        plt.axhline(y=positive_ratio, color='r', linestyle='--',
                    label=f'Random (AP = {positive_ratio:.4f})')

        plt.xlim([0.0, 1.0])
        plt.ylim([0.0, 1.05])
        plt.xlabel('Recall')
        plt.ylabel('Precision')
        plt.title('Precision-Recall Curve')
        plt.legend(loc="lower left")
        plt.grid(True, alpha=0.3)

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º
        save_path = self.results_dir / 'precision_recall_curve.png'
        plt.savefig(save_path, dpi=150, bbox_inches='tight')
        plt.close()

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–∞–Ω–Ω—ã–µ - –û–ë–ù–û–í–õ–ï–ù–û: –∏—Å–ø—Ä–∞–≤–ª—è–µ–º —Ä–∞–∑–º–µ—Ä—ã
        # precision –∏ recall —Å–æ–¥–µ—Ä–∂–∞—Ç –Ω–∞ 1 —ç–ª–µ–º–µ–Ω—Ç –±–æ–ª—å—à–µ —á–µ–º thresholds
        if len(precision) == len(recall):
            if len(thresholds) == len(precision) - 1:
                # –°–æ–∑–¥–∞–µ–º DataFrame –±–µ–∑ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ —ç–ª–µ–º–µ–Ω—Ç–∞ precision –∏ recall
                pr_data = pd.DataFrame({
                    'precision': precision[:-1],
                    'recall': recall[:-1],
                    'thresholds': thresholds
                })
            elif len(thresholds) == len(precision):
                # –†–∞–∑–º–µ—Ä—ã —É–∂–µ —Å–æ–≤–ø–∞–¥–∞—é—Ç
                pr_data = pd.DataFrame({
                    'precision': precision,
                    'recall': recall,
                    'thresholds': thresholds
                })
            else:
                # –ù–µ–ø—Ä–µ–¥–≤–∏–¥–µ–Ω–Ω—ã–π —Å–ª—É—á–∞–π - —Å–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–∞–∑–¥–µ–ª—å–Ω–æ
                logger.warning(
                    f"–ù–µ—Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–æ–≤: precision={len(precision)}, recall={len(recall)}, thresholds={len(thresholds)}")
                pr_data = pd.DataFrame({
                    'precision': precision,
                    'recall': recall
                })
                pr_data.to_csv(self.results_dir / 'precision_recall_data.csv', index=False)
                pd.DataFrame({'thresholds': thresholds}).to_csv(
                    self.results_dir / 'pr_thresholds.csv', index=False)
                return save_path

            pr_data.to_csv(self.results_dir / 'precision_recall_data.csv', index=False)
        else:
            logger.warning(f"–†–∞–∑–º–µ—Ä—ã precision –∏ recall –Ω–µ —Å–æ–≤–ø–∞–¥–∞—é—Ç: precision={len(precision)}, recall={len(recall)}")

        logger.info(f"Precision-Recall –∫—Ä–∏–≤–∞—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {save_path}")

        return save_path

    def plot_confusion_matrix(self, y_true, y_pred, metrics):
        """–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –º–∞—Ç—Ä–∏—Ü—ã –æ—à–∏–±–æ–∫"""
        plt.figure(figsize=(10, 8))

        cm = confusion_matrix(y_true, y_pred)

        # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                    xticklabels=['Bird', 'Drone'],
                    yticklabels=['Bird', 'Drone'])

        plt.title('Confusion Matrix')
        plt.xlabel('Predicted Label')
        plt.ylabel('True Label')

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º
        save_path = self.results_dir / 'confusion_matrix.png'
        plt.savefig(save_path, dpi=150, bbox_inches='tight')
        plt.close()

        logger.info(f"–ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {save_path}")

        return save_path

    def plot_calibration_curve(self, y_true, y_proba):
        """–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –∫–∞–ª–∏–±—Ä–æ–≤–æ—á–Ω–æ–π –∫—Ä–∏–≤–æ–π"""
        plt.figure(figsize=(10, 8))

        prob_true, prob_pred = calibration_curve(y_true, y_proba[:, 1], n_bins=10)

        plt.plot(prob_pred, prob_true, marker='o', linewidth=1, label='Classifier')
        plt.plot([0, 1], [0, 1], linestyle='--', label='Perfectly calibrated')

        plt.xlabel('Mean predicted probability')
        plt.ylabel('Fraction of positives')
        plt.title('Calibration Curve (Reliability Diagram)')
        plt.legend(loc="lower right")
        plt.grid(True, alpha=0.3)

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º
        save_path = self.results_dir / 'calibration_curve.png'
        plt.savefig(save_path, dpi=150, bbox_inches='tight')
        plt.close()

        logger.info(f"–ö–∞–ª–∏–±—Ä–æ–≤–æ—á–Ω–∞—è –∫—Ä–∏–≤–∞—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {save_path}")

        return save_path

    def plot_class_distributions(self, y_true, y_proba):
        """–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–π –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –ø–æ –∫–ª–∞—Å—Å–∞–º"""
        fig, axes = plt.subplots(1, 2, figsize=(14, 6))

        # –î–ª—è –ø—Ç–∏—Ü (–∫–ª–∞—Å—Å 0)
        bird_probs = y_proba[y_true == 0, 1]  # –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –±—ã—Ç—å –¥—Ä–æ–Ω–æ–º
        if len(bird_probs) > 0:
            axes[0].hist(bird_probs, bins=20, alpha=0.7, color='skyblue', edgecolor='black')
            axes[0].set_title('Prediction Distribution for Birds (True Class 0)')
            axes[0].set_xlabel('Predicted Probability (Drone)')
            axes[0].set_ylabel('Frequency')
            axes[0].grid(True, alpha=0.3)
            axes[0].axvline(0.5, color='red', linestyle='--', label='Decision threshold')
            axes[0].legend()
        else:
            axes[0].text(0.5, 0.5, 'No bird samples', ha='center', va='center')
            axes[0].set_title('No Bird Samples')

        # –î–ª—è –¥—Ä–æ–Ω–æ–≤ (–∫–ª–∞—Å—Å 1)
        drone_probs = y_proba[y_true == 1, 1]  # –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –±—ã—Ç—å –¥—Ä–æ–Ω–æ–º
        if len(drone_probs) > 0:
            axes[1].hist(drone_probs, bins=20, alpha=0.7, color='lightcoral', edgecolor='black')
            axes[1].set_title('Prediction Distribution for Drones (True Class 1)')
            axes[1].set_xlabel('Predicted Probability (Drone)')
            axes[1].set_ylabel('Frequency')
            axes[1].grid(True, alpha=0.3)
            axes[1].axvline(0.5, color='red', linestyle='--', label='Decision threshold')
            axes[1].legend()
        else:
            axes[1].text(0.5, 0.5, 'No drone samples', ha='center', va='center')
            axes[1].set_title('No Drone Samples')

        plt.tight_layout()

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º
        save_path = self.results_dir / 'class_distributions.png'
        plt.savefig(save_path, dpi=150, bbox_inches='tight')
        plt.close()

        logger.info(f"–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –ø–æ –∫–ª–∞—Å—Å–∞–º —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {save_path}")

        return save_path

    def save_metrics(self, metrics, y_true, y_pred, y_proba):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤—Å–µ—Ö –º–µ—Ç—Ä–∏–∫ –∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"""

        # 1. –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç—Ä–∏–∫–∏ –≤ JSON
        import json

        def convert_to_serializable(obj):
            if isinstance(obj, np.integer):
                return int(obj)
            elif isinstance(obj, np.floating):
                return float(obj)
            elif isinstance(obj, np.ndarray):
                return obj.tolist()
            elif isinstance(obj, dict):
                return {k: convert_to_serializable(v) for k, v in obj.items()}
            elif isinstance(obj, list):
                return [convert_to_serializable(item) for item in obj]
            else:
                return obj

        serializable_metrics = convert_to_serializable(metrics)

        metrics_path = self.results_dir / 'metrics.json'
        with open(metrics_path, 'w') as f:
            json.dump(serializable_metrics, f, indent=4)

        logger.info(f"–ú–µ—Ç—Ä–∏–∫–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {metrics_path}")

        # 2. –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –≤ CSV
        if len(y_true) > 0:
            predictions_df = pd.DataFrame({
                'true_label': y_true,
                'predicted_label': y_pred,
                'probability_bird': y_proba[:, 0],
                'probability_drone': y_proba[:, 1],
                'correct': (y_true == y_pred).astype(int)
            })

            predictions_path = self.results_dir / 'predictions.csv'
            predictions_df.to_csv(predictions_path, index=False)

            logger.info(f"–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {predictions_path}")
        else:
            logger.warning("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π")

        # 3. –°–æ–∑–¥–∞–µ–º —Ç–µ–∫—Å—Ç–æ–≤—ã–π –æ—Ç—á–µ—Ç
        report_path = self.results_dir / 'evaluation_report.txt'

        with open(report_path, 'w') as f:
            f.write("=" * 70 + "\n")
            f.write("EVALUATION REPORT\n")
            f.write("=" * 70 + "\n\n")

            f.write(f"Model: {self.model_path}\n")
            f.write(f"Evaluation Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"Device: {self.device}\n\n")

            f.write(f"Validation Dataset:\n")
            f.write(f"  Total samples: {metrics.get('num_samples', 0)}\n")
            f.write(f"  Birds (0): {metrics.get('num_birds', 0)}\n")
            f.write(f"  Drones (1): {metrics.get('num_drones', 0)}\n\n")

            f.write("-" * 70 + "\n")
            f.write("PERFORMANCE METRICS\n")
            f.write("-" * 70 + "\n\n")

            f.write(f"Accuracy:           {metrics.get('accuracy', 0):.4f}\n")
            f.write(f"Precision:          {metrics.get('precision', 0):.4f}\n")
            f.write(f"Recall:             {metrics.get('recall', 0):.4f}\n")
            f.write(f"F1-Score:           {metrics.get('f1_score', 0):.4f}\n")
            f.write(f"ROC-AUC:            {metrics.get('roc_auc', 0):.4f}\n")
            f.write(f"PR-AUC:             {metrics.get('pr_auc', 0):.4f}\n\n")

            if 'true_positive' in metrics:
                f.write(f"True Positives:     {metrics['true_positive']}\n")
                f.write(f"True Negatives:     {metrics['true_negative']}\n")
                f.write(f"False Positives:    {metrics['false_positive']}\n")
                f.write(f"False Negatives:    {metrics['false_negative']}\n\n")

                f.write(f"True Positive Rate:  {metrics.get('true_positive_rate', 0):.4f}\n")
                f.write(f"True Negative Rate:  {metrics.get('true_negative_rate', 0):.4f}\n")
                f.write(f"False Positive Rate: {metrics.get('false_positive_rate', 0):.4f}\n")
                f.write(f"False Negative Rate: {metrics.get('false_negative_rate', 0):.4f}\n\n")

            f.write("-" * 70 + "\n")
            f.write("CLASSIFICATION REPORT\n")
            f.write("-" * 70 + "\n\n")

            report = metrics.get('classification_report', {})
            for class_name in ['Bird', 'Drone', 'macro avg', 'weighted avg']:
                if class_name in report:
                    f.write(f"{class_name}:\n")
                    for metric in ['precision', 'recall', 'f1-score', 'support']:
                        if metric in report[class_name]:
                            f.write(f"  {metric}: {report[class_name][metric]:.4f}\n")
                    f.write("\n")

        logger.info(f"–¢–µ–∫—Å—Ç–æ–≤—ã–π –æ—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {report_path}")

        return metrics_path, predictions_path if 'predictions_df' in locals() else None, report_path

    def print_summary(self, metrics):
        """–í—ã–≤–æ–¥ —Å–≤–æ–¥–∫–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"""
        print("\n" + "=" * 80)
        print("MODEL EVALUATION SUMMARY")
        print("=" * 80)

        print(f"\nüìä BASIC METRICS:")
        print(f"   Accuracy:          {metrics.get('accuracy', 0):.4f}")
        print(f"   Precision:         {metrics.get('precision', 0):.4f}")
        print(f"   Recall:            {metrics.get('recall', 0):.4f}")
        print(f"   F1-Score:          {metrics.get('f1_score', 0):.4f}")

        print(f"\nüéØ AUC METRICS:")
        print(f"   ROC-AUC:           {metrics.get('roc_auc', 0):.4f}")
        print(f"   PR-AUC:            {metrics.get('pr_auc', 0):.4f}")

        print(f"\nüìà CONFUSION MATRIX:")
        cm = metrics.get('confusion_matrix', [])
        if len(cm) == 2 and len(cm[0]) == 2:
            print(f"   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê")
            print(f"   ‚îÇ   Predicted ‚îÇ Bird  Drone ‚îÇ")
            print(f"   ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§")
            print(f"   ‚îÇ Actual Bird ‚îÇ {cm[0][0]:^5}  {cm[0][1]:^5} ‚îÇ")
            print(f"   ‚îÇ Actual Drone‚îÇ {cm[1][0]:^5}  {cm[1][1]:^5} ‚îÇ")
            print(f"   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò")
        else:
            print(f"   –ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞ –∏–ª–∏ –∏–º–µ–µ—Ç –Ω–µ–≤–µ—Ä–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç")

        print(f"\nüìã DETAILS:")
        report = metrics.get('classification_report', {})
        if 'Bird' in report:
            print(f"   Bird Class - Precision: {report['Bird']['precision']:.4f}, "
                  f"Recall: {report['Bird']['recall']:.4f}, "
                  f"F1: {report['Bird']['f1-score']:.4f}")
        if 'Drone' in report:
            print(f"   Drone Class - Precision: {report['Drone']['precision']:.4f}, "
                  f"Recall: {report['Drone']['recall']:.4f}, "
                  f"F1: {report['Drone']['f1-score']:.4f}")

        print(f"\nüíæ All results saved to: {self.results_dir}")
        print("=" * 80)

    def evaluate(self):
        """–û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –æ—Ü–µ–Ω–∫–∏"""
        logger.info("–ó–∞–ø—É—Å–∫ –æ—Ü–µ–Ω–∫–∏ –º–æ–¥–µ–ª–∏...")

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –¥–∞–Ω–Ω—ã–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã
        if self.val_loader is None:
            logger.error("–í–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω!")
            return {}

        # 1. –ü–æ–ª—É—á–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
        y_true, y_pred, y_proba = self.get_predictions()

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –µ—Å—Ç—å –¥–∞–Ω–Ω—ã–µ
        if len(y_true) == 0:
            logger.error("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ—Ü–µ–Ω–∫–∏!")
            return {}

        # 2. –í—ã—á–∏—Å–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏
        metrics = self.calculate_metrics(y_true, y_pred, y_proba)

        # 3. –°—Ç—Ä–æ–∏–º –≥—Ä–∞—Ñ–∏–∫–∏ —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –µ—Å—Ç—å –¥–∞–Ω–Ω—ã–µ
        try:
            if 'roc_auc' in metrics and metrics['roc_auc'] > 0:
                self.plot_roc_curve(y_true, y_proba, metrics['roc_auc'])

            if 'pr_auc' in metrics and metrics['pr_auc'] > 0:
                self.plot_precision_recall_curve(y_true, y_proba, metrics['pr_auc'])

            if 'confusion_matrix' in metrics and len(metrics['confusion_matrix']) > 0:
                self.plot_confusion_matrix(y_true, y_pred, metrics)

            self.plot_calibration_curve(y_true, y_proba)
            self.plot_class_distributions(y_true, y_proba)

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏–∏ –≥—Ä–∞—Ñ–∏–∫–æ–≤: {e}")

        # 4. –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        try:
            self.save_metrics(metrics, y_true, y_pred, y_proba)
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤: {e}")

        # 5. –í—ã–≤–æ–¥–∏–º —Å–≤–æ–¥–∫—É
        self.print_summary(metrics)

        logger.info("–û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏ –∑–∞–≤–µ—Ä—à–µ–Ω–∞!")

        return metrics


def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    parser = argparse.ArgumentParser(description='–û—Ü–µ–Ω–∫–∞ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞ –ø—Ç–∏—Ü/–¥—Ä–æ–Ω–æ–≤')

    parser.add_argument('--model_path', type=str, default=r'weights/mobilenet_encoder 32/final_model.pth',
                        help='–ü—É—Ç—å –∫ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ (.pth —Ñ–∞–π–ª)')

    parser.add_argument('--data_dir', type=str, default=None,
                        help='–ü—É—Ç—å –∫ –¥–∞–Ω–Ω—ã–º (–µ—Å–ª–∏ None, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è Config.data_dir)')

    parser.add_argument('--batch_size', type=int, default=32,
                        help='–†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ –¥–ª—è –æ—Ü–µ–Ω–∫–∏')

    parser.add_argument('--results_dir', type=str, default='TF_encoder_evaluation',
                        help='–ü–∞–ø–∫–∞ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤')

    args = parser.parse_args()

    try:
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
        config = Config()

        # –û–±–Ω–æ–≤–ª—è–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –∏–∑ –∞—Ä–≥—É–º–µ–Ω—Ç–æ–≤
        if args.data_dir:
            config.data_dir = args.data_dir
        if args.batch_size:
            config.batch_size = args.batch_size

        # –°–æ–∑–¥–∞–µ–º –æ—Ü–µ–Ω—â–∏–∫
        evaluator = ModelEvaluator(
            model_path=args.model_path,
            config=config,
            results_dir=args.results_dir
        )

        # –ó–∞–ø—É—Å–∫–∞–µ–º –æ—Ü–µ–Ω–∫—É
        metrics = evaluator.evaluate()

        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º ROC-AUC
        print(f"\nüéØ ROC-AUC Score: {metrics.get('roc_auc', 0):.4f}")

        return metrics

    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ—Ü–µ–Ω–∫–µ –º–æ–¥–µ–ª–∏: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()